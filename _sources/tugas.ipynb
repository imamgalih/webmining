{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqFh/XnkfTuBg8gdQ/nKVC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"We3kl8nTuMUF"},"source":["# **Data crawling**\n","Data crawling atau perayapan data adalah proses pengambilan data yang tersedia secara online untuk umum. Proses ini kemudian mengimpor informasi atau data yang telah ditemukan ke dalam file lokal di komputer Anda."]},{"cell_type":"markdown","metadata":{"id":"nAb2TIcEu-81"},"source":["## Twint\n","\n","Twint adalah sebuah tools yang digunakan untuk melakukan scrapping dari aplikasi twitter yang disetting secara khusus menggunakan bahasa pemrograman Python. Twint dapat kita gunakan dan jalankan tanpa harus menggunakan API dari Twitter itu sendiri, dengan kapasitas scrapping data maksimalnya adalah 3200 tweet.\n","\n","Bukan hanya digunakan pada tweet, twint juga bisa kita gunakan untuk melakukan scrapping pada user, followers, retweet dan sebagainya. Twint memanfaatkan operator pencarian twitter untuk memungkinkan proses penghapusan tweet dari user tertentu, memilih dan memilah informasi-informasi yang sensitif, termasuk email dan nomor telepon di dalamnya."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFH7xVwxUhEC"},"outputs":[],"source":["# !git clone --depth=1 https://github.com/twintproject/twint.git\n","# %cd twint\n","# !pip3 install . -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2K-DUnBDdU_"},"outputs":[],"source":["# pip install nest-asyncio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkvP5Mt3DeJ1"},"outputs":[],"source":["# !pip install aiohttp==3.7.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-A1_9INDgUQ","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1670924587504,"user_tz":-420,"elapsed":129,"user":{"displayName":"19-024 Imam Galih Setiono","userId":"01128849358429055601"}},"outputId":"2e51edbb-397e-4c26-8a27-b3c13472d289"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-73209377f4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#digunakan sekali untuk mengaktifkan tindakan serentak dalam notebook jupyter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtwint\u001b[0m \u001b[0;31m#untuk import twint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'puan maharani'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nest_asyncio'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import nest_asyncio\n","nest_asyncio.apply() #digunakan sekali untuk mengaktifkan tindakan serentak dalam notebook jupyter.\n","import twint #untuk import twint\n","c = twint.Config()\n","c.Search = 'puan maharani'\n","c.Pandas = True\n","c.Limit = 70\n","twint.run.Search(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AW4keKjdDkwm"},"outputs":[],"source":["Tweets_dfs = twint.storage.panda.Tweets_df\n","Tweets_dfs[\"tweet\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_YZ8M09DnZG"},"outputs":[],"source":["Tweets_dfs[\"tweet\"].to_csv(\"puan.csv\")"]},{"cell_type":"markdown","metadata":{"id":"kfhORp51vW00"},"source":["# **Mengambil Data**\n","\n","Proses ini digunakan untuk mengambil 100 data tweet yang telah disimpan dalam github dengan format .csv\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSIOdhmaLkMf"},"outputs":[],"source":["#install library pandas\n","!pip install pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdbqJE7nLn0f"},"outputs":[],"source":["#install library numpy\n","!pip install numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_QNdvwcLt8B"},"outputs":[],"source":["import pandas as pd \n","import numpy as np\n","\n","data_abstrak = pd.read_csv(\"https://raw.githubusercontent.com/Kaffin190176/file_tugas/main/puan%20(3)%20(1)%20(2).csv\")\n","\n","data_abstrak"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlV9RgKLkOHg"},"outputs":[],"source":["#install library sastrawi\n","!pip install sastrawi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2owXZ4DqnbEu"},"outputs":[],"source":["#install library swifter\n","!pip install swifter"]},{"cell_type":"markdown","metadata":{"id":"KdrHBoVoxWj-"},"source":["# *Case Folding**\n","\n","Tahap untuk merubah teks yang memiliki huruf kapital menjadi huruf kecil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZb_8O0OMBc5"},"outputs":[],"source":["data_abstrak['Abstrac'] = data_abstrak['Abstrac'].str.lower()\n","\n","\n","data_abstrak['Abstrac']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrS6ADokZLfV"},"outputs":[],"source":["#install library nltk\n","!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"GidSZu_2ymdP"},"source":["## Menghapus Karakter Spesial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PfMxOqmamxD"},"outputs":[],"source":["import string \n","import re #regex library\n","# import word_tokenize & FreqDist from NLTK\n","\n","from nltk.tokenize import word_tokenize \n","from nltk.probability import FreqDist\n","\n","# ------ Tokenizing ---------\n","\n","def remove_special(text):\n","    # remove tab, new line, ans back slice\n","    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\" \").replace('\\\\f',\" \").replace('\\\\r',\" \")\n","    # remove non ASCII (emoticon, chinese word, .etc)\n","    text = text.encode('ascii', 'replace').decode('ascii')\n","    # remove mention, link, hashtag\n","    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","    # remove incomplete URL\n","    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n","                \n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(remove_special)\n","data_abstrak['Abstrac']\n"]},{"cell_type":"markdown","metadata":{"id":"4aaueWDLzwNm"},"source":["## Menghapus Angka"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWoNw6CsywxF"},"outputs":[],"source":["\n","#remove number\n","def remove_number(text):\n","    return  re.sub(r\"\\d+\", \"\", text)\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(remove_number)\n","data_abstrak['Abstrac']"]},{"cell_type":"markdown","metadata":{"id":"pY-4fnONz2Es"},"source":["## Menghapus Tanda Baca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCr9v6zey0QJ"},"outputs":[],"source":["\n","#remove punctuation\n","def remove_punctuation(text):\n","    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(remove_punctuation)\n","data_abstrak['Abstrac']"]},{"cell_type":"markdown","metadata":{"id":"zQQ9-N1_0Bvz"},"source":["## Menghapus Spasi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFo_LuODy2n0"},"outputs":[],"source":["\n","#remove whitespace leading & trailing\n","def remove_whitespace_LT(text):\n","    return text.strip()\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(remove_whitespace_LT)\n","\n","\n","#remove multiple whitespace into single whitespace\n","def remove_whitespace_multiple(text):\n","    return re.sub('\\s+',' ',text)\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(remove_whitespace_multiple)\n","data_abstrak['Abstrac']"]},{"cell_type":"markdown","metadata":{"id":"kNKKQN460Vz0"},"source":["## Menghapus huruf "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UWUcE1ly9M1"},"outputs":[],"source":["\n","# remove single char\n","def remove_singl_char(text):\n","    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(remove_singl_char)\n","data_abstrak['Abstrac']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T30oghHxx2En"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"ncy4Kc0h0gjs"},"source":["# **Tokenizing**\n","\n","Tokenizing adalah proses pemisahan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian di analisa. Kata, angka, simbol, tanda baca dan entitas penting lainnya dapat dianggap sebagai token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrAM55OOy-t9"},"outputs":[],"source":["\n","# NLTK word Tokenize \n","def word_tokenize_wrapper(text):\n","    return word_tokenize(text)\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(word_tokenize_wrapper)\n","data_abstrak['Abstrac']"]},{"cell_type":"markdown","metadata":{"id":"LRII2cK-1XPc"},"source":["# **Filtering(Stopwords Removal)**\n","Proses untuk menghapus kata hubung atau kata yang tidak memiliki makna\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyR-Rhpnijqv"},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9QuAKmgZ6py"},"outputs":[],"source":["\n","list_stopwords = stopwords.words('indonesian')\n","\n","#Menghapus Stopword dari list token\n","def stopwords_removal(words):\n","    return [word for word in words if word not in list_stopwords]\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].apply(stopwords_removal)\n","\n","data_abstrak['Abstrac']"]},{"cell_type":"markdown","metadata":{"id":"YuqMFVZa3H_d"},"source":["# **Stemming**\n","Data training hasil dari filtering akan dilakukan pengecekan atau pencarian kata-kata yang sesuai dengan kamus umum. Apabila data training hasil filtering sesuai dengan kamus umum maka kata akan dikeluarkan sementara, karena sudah dianggap sebagai kata dasar. Apabila masih terdapat kata yang tidak termasuk dalam kata dasar maka tahap selanjutnya adalah menghapus inflection suffixes yang merupakan akhiran pertama. Kata yang memiliki akhiran partticles seperti “-pun”, “-kah”, “-tah”, “- lah” dan akhiran possessive pronoun seperti “-mu”, “-ku” dan “-nya” dihilangkan. Setelah dilakukan proses case folding, tokenezing, dan filtering, proses selanjutnya yaitu stemming. Stemming yang digunakan pada penelitian ini menggunakan algoritma Enhanced Confix Stipping Stemmer, terdiri dari beberapa langkah: Data training hasil dari filtering akan dilakukan pengecekan atau pencarian kata-kata yang sesuai dengan kamus umum. Apabila data training hasil filtering sesuai dengan kamus umum maka kata akan dikeluarkan sementara, karena sudah dianggap sebagai kata dasar. Apabila masih terdapat kata yang tidak termasuk dalam kata dasar maka tahap selanjutnya adalah menghapus inflection suffixes yang merupakan akhiran pertama. Kata yang memiliki akhiran partticles seperti “-pun”, “-kah”, “-tah”, “- lah” dan akhiran possessive pronoun seperti “-mu”, “-ku” dan “-nya” dihilangkan."]},{"cell_type":"code","source":["from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","import swifter\n","\n","\n","# create stemmer\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","# stemmed\n","def stemmed_wrapper(term):\n","    return stemmer.stem(term)\n","\n","term_dict = {}\n","\n","for document in data_abstrak['Abstrac']:\n","    for term in document:\n","        if term not in term_dict:\n","            term_dict[term] = ' '\n","            \n","print(len(term_dict))\n","print(\"------------------------\")\n"],"metadata":{"id":"NIUkj010adSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for term in term_dict:\n","    term_dict[term] = stemmed_wrapper(term)\n","    print(term,\":\" ,term_dict[term])\n","    \n","print(term_dict)\n","print(\"------------------------\")\n","\n","\n","# apply stemmed term to dataframe\n","def get_stemmed_term(document):\n","    return [term_dict[term] for term in document]\n","\n","data_abstrak['Abstrac'] = data_abstrak['Abstrac'].swifter.apply(get_stemmed_term)\n","data_abstrak['Abstrac']"],"metadata":{"id":"2Y4s8PsVcZK-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jfa1PWdV4RC9"},"source":["##Menyimpan Hasil Tahap Preprocessing ke file .csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IO8wy-TV8Dk"},"outputs":[],"source":["\n","data_abstrak.to_csv('preprocessing.csv')"]},{"cell_type":"markdown","metadata":{"id":"KF8PKplK4pqu"},"source":["# **TF**\n","\n","TF(Term Frequency) : Istilah frekuensi kata dalam dokumen. Ada beberapa cara untuk menghitung frekuensi ini, dengan cara yang paling sederhana adalah dengan menghitung jumlah kata yang muncul dalam dokumen. Lalu, ada cara untuk menyesuaikan frekuensi, berdasarkan panjang dokumen, atau dengan frekuensi mentah kata yang paling sering muncul dalam dokumen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Qda3OpcSCq2"},"outputs":[],"source":["\n","from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n","#Membuat Dataframe\n","dataTextPre = pd.read_csv('preprocessing.csv')\n","vectorizer = CountVectorizer(min_df=1)\n","bag = vectorizer.fit_transform(dataTextPre['Abstrac'])\n","dataTextPre"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6hjdegpSYqN"},"outputs":[],"source":["matrik_vsm=bag.toarray()\n","#print(matrik_vsm)\n","matrik_vsm.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4iujuMwS0It"},"outputs":[],"source":["matrik_vsm[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6GcpBXy8cYI"},"outputs":[],"source":["a=vectorizer.get_feature_names()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RL5lPUR8d6G"},"outputs":[],"source":["print(len(matrik_vsm[:,1]))\n","#dfb =pd.DataFrame(data=matrik_vsm,index=df,columns=[a])\n","dataTF =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a])\n","dataTF"]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n","DataTFIDF = TfidfVectorizer()\n","TFIDF = DataTFIDF.fit_transform(dataTextPre['Abstrac']).toarray()\n","TFIDF = pd.DataFrame(TFIDF)\n","TFIDF"],"metadata":{"id":"Y7mv3OjJoV2Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# KMeans(Clustering)\n"],"metadata":{"id":"oY6Vxq9qphjh"}},{"cell_type":"code","source":[],"metadata":{"id":"lEKfDUIWLfD0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan unssupervised learning dan menggunakan metode yang mengelompokan data berbagai partisi.\n","\n","K Means Clustering memiliki objective yaitu meminimalisasi object function yang telah di atur pada proses clasterisasi. Dengan cara minimalisasi variasi antar 1 cluster dengan maksimalisasi variasi dengan data di cluster lainnya."],"metadata":{"id":"ZYmNkyUzplf5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLWjSWS7wBmj"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","kmeans =KMeans(n_clusters=3)\n","kmeans=kmeans.fit(dataTF)\n","prediksi=kmeans.predict(dataTF)\n","centroids = kmeans.cluster_centers_\n","\n","data=pd.DataFrame(prediksi,columns=[\"Cluster\"])\n","data\n"]},{"cell_type":"markdown","source":["Menambah Label pada Hasil TF"],"metadata":{"id":"0kCJnztlp_6J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4IQgGxNoUJG"},"outputs":[],"source":["datalabel = pd.read_csv('https://raw.githubusercontent.com/Kaffin190176/file_tugas/main/puan%20(3)%20(1)%20(2).csv')\n","dataJurnal = pd.concat([dataTF.reset_index(drop=True), datalabel[\"label\"]], axis=1)\n","dataJurnal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hlIbmOvsOfd"},"outputs":[],"source":["dataJurnal['label'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oOxuqzzspqD"},"outputs":[],"source":["dataJurnal.info()"]},{"cell_type":"markdown","source":["### Split Data"],"metadata":{"id":"6dsiaV8fqNAK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_R05JYzZsn39"},"outputs":[],"source":["### Train test split to avoid overfitting\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(dataJurnal.drop(labels=['label'], axis=1),\n","    dataJurnal['label'],\n","    test_size=0.3,\n","    random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jc35b3YmtBFW"},"outputs":[],"source":["X_train"]},{"cell_type":"markdown","source":["# KNN (Analisis Sentimen)"],"metadata":{"id":"Xd_u54nVqcuJ"}},{"cell_type":"markdown","source":["KNN adalah model sederhana untuk tugas-tugas regresi dan klasifikasi. Ketetanggan adalah representasi dari contoh pelatihan dalam ruang metrik. Ruang metrik adalah ruang fitur di mana jarak antara semua anggota set didefinisikan. Berkaitan dengan masalah pizza pada bab sebelumnya, contoh data latih diwakili dalam ruang metrik karena jarak antara semua diameter pizza ditentukan. Ketetanggan ini digunakan untuk memperkirakan nilai variabel respon untuk contoh uji. Hyperparameter k menentukan berapa banyak tetangga yang dapat digunakan dalam estimasi. Hyperparameter adalah parameter yang mengontrol bagaimana algoritma belajar; hiperparameter tidak diperkirakan dari data pelatihan dan kadang-kadang ditetapkan secara manual. Akhirnya, k tetangga yang dipilih adalah yang terdekat dengan instan uji, yang diukur dengan beberapa fungsi jarak."],"metadata":{"id":"mgMwLhPDqWe6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTIp-NYKOKm8"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","testing=[]\n","listnum=[]\n","for i in range(2,21):\n","  listnum.append(i)\n","  neigh = KNeighborsClassifier(n_neighbors=i)\n","  neigh.fit(X_train, y_train)\n","  Y_pred = neigh.predict(X_test) \n","  testing.append(Y_pred)\n","testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GinsZ2_4P-Yq"},"outputs":[],"source":["y_test"]},{"cell_type":"markdown","metadata":{"id":"VUc3qesDOMmC"},"source":["##hasil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Fjfhu5eRhs5"},"outputs":[],"source":["from sklearn.metrics import make_scorer, accuracy_score,precision_score\n","listtest=[]\n","listacc=[]\n","for i in range(len(testing)):\n","  accuracy_neigh=round(accuracy_score(y_test,testing[i])* 100, 2)\n","  acc_neigh = round(neigh.score(X_train, y_train) * 100, 2)\n","  listappend=listnum[i]\n","  appendlist=listappend,accuracy_neigh\n","  listtest.append(appendlist)\n","  listacc.append(accuracy_neigh)\n","listtest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mL3BVmvUuVQ7"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","plt.bar(listnum, listacc)\n","plt.xticks(listnum)\n","plt.title('Nilai Akurasi Berdasarkan Input')\n","plt.ylabel('Persentase Akurasi')"]},{"cell_type":"markdown","source":["# Topic Modeling"],"metadata":{"id":"1Tgkfr3gr1yZ"}},{"cell_type":"markdown","source":["Tipic Modeling adalah cara mengelompokan data text berdasarkan suatu topik tertentu. Memiliki tujuan yang sama dengan klasifikasi tetapi menggunakan pendekatan berbeda topik modelling merupakan unsupervised learning alias tidak membutuhkan data berlabel. bisa dikatakan topic modelling bekerja seperti clustering dengan mengelompokan dokumen berdasarkan kemiripanya, tetapi topic modelling mempunyai tujuan yang lebih spesifik yaitu :\n","\n","1. Menemukan pola topik abstrak pada kumpulan dokumen\n","2. Memberikan anotasi dokumen berdasarkan topik tersebut\n","3. Menggunakan Anotasi dokumen untuk mengelompokan dokumen"],"metadata":{"id":"EAtPFS4-LIBq"}},{"cell_type":"markdown","source":["## Information Gain"],"metadata":{"id":"H0GeuLB2tvNr"}},{"cell_type":"markdown","source":["Mutual Information atau Information Gain adalah salah satu metode dari seleksi fitur, dalam proses Information Gain fitur akan diranking, ranking fitur yang terbesar merupakan fitur yang paling relevan dan memiliki koneksi yang kuat dengan kumpulan data yang terkait "],"metadata":{"id":"uparZeoGsDaw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWwlnB9rtFYs"},"outputs":[],"source":["from sklearn.feature_selection import mutual_info_classif\n","# determine the mutual information\n","mutual_info = mutual_info_classif(X_train, y_train)\n","mutual_info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Pw-KWsstTeM"},"outputs":[],"source":["mutual_info = pd.Series(mutual_info)\n","mutual_info.index = X_train.columns\n","mutual_info.sort_values(ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUpXiR8IxsFX"},"outputs":[],"source":["#let's plot the ordered mutual_info values per feature\n","mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKm54BR5x2FV"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uCOqr99x7vV"},"outputs":[],"source":["#No we Will select the  top 5 important features\n","sel_five_cols = SelectKBest(mutual_info_classif, k=100)\n","sel_five_cols.fit(X_train, y_train)\n","X_train.columns[sel_five_cols.get_support()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMcvnRyuOA0L"},"outputs":[],"source":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"iARtmrgi6-89"}},{"cell_type":"markdown","source":["Model LSA"],"metadata":{"id":"JCjReKPjna-p"}},{"cell_type":"code","source":["from sklearn.decomposition import TruncatedSVD\n","#Membuat LSA Model\n","lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n","#Ft Transform LSA Model dari TF-IDF\n","lsa_top=lsa_model.fit_transform(TFIDF)"],"metadata":{"id":"I-U8g1SH7BmH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Menampilkan LSA top 10\n","print(lsa_top)\n","print(lsa_top.shape)"],"metadata":{"id":"Lx0mu_QangA7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan Kekuatan Topik pada Setiap Dokumen"],"metadata":{"id":"mSVTtFn0pHOV"}},{"cell_type":"code","source":["for Dokumen in range(TFIDF.shape[0]):\n","    print(\"\\nDokumen : \", Dokumen)\n","    l=lsa_top[Dokumen]\n","    for i,topik in enumerate(l):\n","        print(\"Topik \",i,\" : \",topik*100)"],"metadata":{"id":"V99gP1ygokp0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mengidentifikasi Komponen Kata Setiap Topik"],"metadata":{"id":"4j2Labm3pPRS"}},{"cell_type":"code","source":["print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n","print(lsa_model.components_)"],"metadata":{"id":"pACaNF_Bor7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Kata penting dari setiap Topik\n","vocab = DataTFIDF.get_feature_names()\n","\n","for i, comp in enumerate(lsa_model.components_):\n","    vocab_comp = zip(vocab, comp)\n","    #Menampilkan 10 kata penting setiap topik\n","    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n","    print(\"Topik \"+str(i)+\": \")\n","    for t in sorted_words:\n","        print(t[0],end=\" \")\n","    print(\"\\n\")"],"metadata":{"id":"3Jyk4dNIox0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ensemble Learning\n"],"metadata":{"id":"mUx2Rp60KpNb"}},{"cell_type":"markdown","source":["Metode ensemble atau metode ansamble adalah algoritma dalam pembelajaran mesin (machine learning) dimana algoritma ini sebagai pencarian solusi prediksi terbaik dibandingkan dengan algoritma yang lain karena metode ensemble ini menggunakan beberapa algoritma pembelajaran untuk pencapaian solusi prediksi yang lebih baik daripada algoritma yang bisa diperoleh dari salah satu pembelajaran algoritma kosituen saja. Tidak seperti ansamble statistika didalam mekanika statistika biasanya selalu tak terbatas. Ansemble Pembelajaran hanya terdiri dari seperangkat model alternatif yang bersifat terbatas, namun biasanya memungkinkan untuk menjadi lebih banyak lagi struktur fleksibel yang ada diantara alternatif model itu sendiri.\n","\n","Evaluasi prediksi dari ensemble biasanya memerlukan banyak komputasi daripada evaluasi prediksi model tunggal (single model), jadi ensemble ini memungkinkan untuk mengimbangi poor learning algorithms oleh performasi lebih dari komputasi itu."],"metadata":{"id":"sQPInjL93Yuv"}},{"cell_type":"code","source":["\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.model_selection import train_test_split\n","\n","estimators = [\n","    ('rf1', RandomForestClassifier(n_estimators=10, random_state=40)),\n","    ('rf2', RandomForestClassifier(n_estimators=5, random_state=40))\n","    \n","    ]\n","clf = StackingClassifier(\n","    estimators=estimators, final_estimator=LogisticRegression()\n","    )\n","\n","clf.fit(X_train, y_train).score(X_test, y_test)"],"metadata":{"id":"PtrkKHsa3b1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","X, y = load_iris(return_X_y=True)\n","X.shape\n"],"metadata":{"id":"uyxujVxB63WV"},"execution_count":null,"outputs":[]}]}